# Double Critic è®­ç»ƒæŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨è®­ç»ƒ

```bash
# åœ¨å¹³å¦åœ°å½¢è®­ç»ƒï¼ˆæµ‹è¯•ç”¨ï¼‰
python scripts/train.py --task=humanoid_ppo --double_critic

# åœ¨ Stones Everywhere åœ°å½¢è®­ç»ƒï¼ˆæ¨èï¼‰
python scripts/train.py --task=humanoid_stones_ppo --double_critic --num_envs=4096 --headless
```

**å¯ç”¨çš„ Task**:
- `humanoid_ppo` - å¹³å¦åœ°å½¢ï¼ˆæµ‹è¯•/è°ƒè¯•ç”¨ï¼‰
- `humanoid_stones_ppo` - Stones Everywhere åœ°å½¢ï¼ˆæ¨èç”¨ double criticï¼‰

### éªŒè¯å®ç°

```bash
python test_double_critic.py
```

---

## ğŸ’¡ æ ¸å¿ƒåŸç†

### ğŸ¯ æ™ºèƒ½çš„ Resume æ”¯æŒ

**ä»å• critic æ¨¡å‹å‡çº§åˆ° double criticï¼Ÿæ²¡é—®é¢˜ï¼**

ä»£ç ä¼šè‡ªåŠ¨ï¼š
1. åŠ è½½ Actor å’Œ Critic1 çš„æƒé‡ âœ…
2. **ç”¨ Critic1 çš„æƒé‡åˆå§‹åŒ– Critic2**ï¼ˆè€Œééšæœºï¼‰âœ…
3. Critic2 ä»ä¸€ä¸ª"èªæ˜"çš„èµ·ç‚¹ç»§ç»­å­¦ä¹  âœ…

**ä¼˜åŠ¿**: Critic2 ç»§æ‰¿ Critic1 çš„çŸ¥è¯†ï¼Œå­¦ä¹ æ›´å¿«ï¼

---

### é—®é¢˜ï¼šç¨€ç–å¥–åŠ±å­¦ä¹ å›°éš¾

ä¼ ç»Ÿå• critic è®­ç»ƒæ—¶ï¼Œ**ç¨€ç–å¥–åŠ±**ï¼ˆå¦‚ footholdï¼‰å®¹æ˜“è¢«**å¯†é›†å¥–åŠ±**ï¼ˆå¦‚é€Ÿåº¦è·Ÿè¸ªï¼‰æ·¹æ²¡ï¼š

```
å• critic:
  æ‰€æœ‰å¥–åŠ± â†’ ä¸€ä¸ª critic â†’ ç¨€ç–å¥–åŠ±ä¿¡å·å¤ªå¼± âŒ
```

### è§£å†³æ–¹æ¡ˆï¼šDouble Critic

ç”¨**ä¸¤ä¸ªç‹¬ç«‹çš„ critic** åˆ†åˆ«å­¦ä¹ ä¸¤ç±»å¥–åŠ±ï¼š

```
Double Critic:
  å¯†é›†å¥–åŠ± (locomotion) â†’ Critic 1 â†’ V1 âœ…
  ç¨€ç–å¥–åŠ± (foothold)   â†’ Critic 2 â†’ V2 âœ…
  
  ç»„åˆ: A = 1.0 * A1 + 0.25 * A2
```

---

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. å¥–åŠ±åˆ†ç¦»

**Dense Rewards (R1)** - å¯†é›†å¥–åŠ±ï¼ˆæ‰€æœ‰ locomotion ç›¸å…³ï¼‰:
- `tracking_vel` - é€Ÿåº¦è·Ÿè¸ª
- `orientation` - å§¿æ€æ§åˆ¶
- `feet_clearance` - æŠ¬è„šé«˜åº¦
- `joint_pos` - å…³èŠ‚ä½ç½®
- `action_smoothness` - åŠ¨ä½œå¹³æ»‘
- ... (é™¤äº† foothold çš„æ‰€æœ‰å¥–åŠ±)

**Sparse Reward (R2)** - ç¨€ç–å¥–åŠ±:
- `foothold` - è½è„šç‚¹å®‰å…¨æ€§ï¼ˆåªåœ¨ stepping stones/beams ä¸Šé‡è¦ï¼‰

### 2. ç½‘ç»œæ¶æ„

```
è¾“å…¥: 272D è§‚æµ‹ (phase, commands, joints, heightmap, ...)
  â”‚
  â”œâ”€ Actor:   [512, 256, 128] â†’ 12D åŠ¨ä½œ
  â”œâ”€ Critic1: [512, 256, 128] â†’ 1D value (é¢„æµ‹ dense å¥–åŠ±çš„ç´¯è®¡)
  â””â”€ Critic2: [512, 256, 128] â†’ 1D value (é¢„æµ‹ sparse å¥–åŠ±çš„ç´¯è®¡)
```

### 3. Advantage è®¡ç®—ï¼ˆæ ¸å¿ƒï¼ï¼‰

```python
# Step 1: åˆ†åˆ«è®¡ç®— GAE (Generalized Advantage Estimation)
for t in reversed(range(T)):
    Î´1[t] = R_dense[t] + Î³*V1[t+1] - V1[t]
    A1[t] = Î´1[t] + Î³*Î»*A1[t+1]
    
    Î´2[t] = R_sparse[t] + Î³*V2[t+1] - V2[t]
    A2[t] = Î´2[t] + Î³*Î»*A2[t+1]

# Step 2: ç‹¬ç«‹å½’ä¸€åŒ–ï¼ˆé˜²æ­¢å°ºåº¦é—®é¢˜ï¼‰
A1_norm = (A1 - mean(A1)) / std(A1)
A2_norm = (A2 - mean(A2)) / std(A2)

# Step 3: åŠ æƒç»„åˆ
A_final = w1 * A1_norm + w2 * A2_norm
        = 1.0 * A1_norm + 0.25 * A2_norm
```

**ä¸ºä»€ä¹ˆ w2=0.25ï¼Ÿ**
- Foothold å¥–åŠ±å¾ˆç¨€ç–ï¼Œå¦‚æœæƒé‡å¤ªå¤§ä¼šå¯¼è‡´ä¸ç¨³å®š
- 0.25 ç¡®ä¿ç¨€ç–å¥–åŠ±æœ‰å½±å“åŠ›ï¼Œä½†ä¸ä¼šä¸»å¯¼è®­ç»ƒ

### 4. Loss è®¡ç®—

```python
# Policy loss (ç”¨ç»„åˆçš„ advantage)
L_policy = PPO_clip_loss(A_final)

# Value loss (ä¸¤ä¸ª critic å„è‡ªçš„ç›®æ ‡)
L_value1 = MSE(V1, returns_dense)   # Critic1 å­¦ä¹ é¢„æµ‹ dense å¥–åŠ±
L_value2 = MSE(V2, returns_sparse)  # Critic2 å­¦ä¹ é¢„æµ‹ sparse å¥–åŠ±

# æ€» loss
L_total = L_policy + coef * (L_value1 + L_value2) - entropy_coef * entropy
```

**é‡è¦ä¿®å¤ï¼ˆå·²å®Œæˆï¼‰ï¼š** 
- âœ… Critic2 ç°åœ¨æ­£ç¡®ä½¿ç”¨ `returns2`ï¼ˆsparse returnsï¼‰ä½œä¸ºè®­ç»ƒç›®æ ‡
- âœ… ä¹‹å‰çš„ç‰ˆæœ¬é”™è¯¯åœ°ä½¿ç”¨äº† `returns`ï¼ˆdense returnsï¼‰ï¼Œå¯¼è‡´ Critic2 å­¦ä¹ ç›®æ ‡ä¸ä¸€è‡´

---

## ğŸ“Š é…ç½®å‚æ•°

### åœ¨ `humanoid_config.py` ä¸­ï¼š

```python
class algorithm(LeggedRobotCfgPPO.algorithm):
    use_double_critic = False           # é€šè¿‡ --double_critic å¯ç”¨
    advantage_weight_dense = 1.0        # Dense å¥–åŠ±æƒé‡
    advantage_weight_sparse = 0.25      # Sparse å¥–åŠ±æƒé‡
```

### è°ƒæ•´æƒé‡çš„å»ºè®®ï¼š

| åœºæ™¯ | w1 (dense) | w2 (sparse) | æ•ˆæœ |
|------|-----------|-------------|------|
| **é»˜è®¤** | 1.0 | 0.25 | å¹³è¡¡ locomotion å’Œ foothold |
| å¼ºè°ƒè½è„šç‚¹ | 1.0 | 0.5 | Foothold å­¦ä¹ æ›´å¿« |
| å¼ºè°ƒç§»åŠ¨ | 1.0 | 0.1 | Locomotion ä¼˜å…ˆ |

---

## ğŸ” å…³é”®ä»£ç è§£æ

### `.clamp()` çš„ä½œç”¨

**PPO çš„ Clipped Value Loss** - é˜²æ­¢ value é¢„æµ‹å˜åŒ–å¤ªå‰§çƒˆï¼š

```python
# ä¸ç”¨ clamp (å±é™©):
# V å¯ä»¥ä» 10.0 çªç„¶è·³åˆ° 100.0 â†’ è®­ç»ƒä¸ç¨³å®š âŒ

# ç”¨ clamp (å®‰å…¨):
# V åªèƒ½ä» 10.0 ç¼“æ…¢å˜åŒ–åˆ° 10.2 â†’ è®­ç»ƒç¨³å®š âœ…

value_clipped = old_value + clamp(new_value - old_value, -0.2, 0.2)
#                           â†‘ é™åˆ¶å˜åŒ–åœ¨ Â±clip_param (0.2) èŒƒå›´å†…

loss = max(
    MSE(new_value, target),      # æ­£å¸¸ loss
    MSE(clipped_value, target)   # é™åˆ¶åçš„ loss
)  # å–æœ€å¤§å€¼ â†’ æ›´ä¿å®ˆçš„æ›´æ–°
```

### è®­ç»ƒå¾ªç¯

```python
# 1. ç¯å¢ƒäº¤äº’
for step in range(num_steps):
    actions = actor(obs)
    obs, rewards, done = env.step(actions)
    
    # åˆ†ç¦»å¥–åŠ±
    rewards_dense = env.rew_buf_dense
    rewards_sparse = env.rew_buf_sparse
    
    # è·å–åŒ value ä¼°è®¡
    V1, V2 = critic1(obs), critic2(obs)
    
    # å­˜å‚¨
    storage.add(obs, actions, rewards_dense, rewards_sparse, V1, V2)

# 2. è®¡ç®— advantage
storage.compute_returns(
    last_V1, last_V2,
    w1=1.0, w2=0.25
)

# 3. æ›´æ–°ç½‘ç»œ
for batch in storage.batches():
    # Policy update
    loss_policy = PPO_loss(batch.advantages)
    
    # Value updates
    V1_new, V2_new = critics(batch.obs)
    loss_V1 = MSE(V1_new, batch.returns_dense)
    loss_V2 = MSE(V2_new, batch.returns_sparse)
    
    # æ€» loss
    loss = loss_policy + loss_V1 + loss_V2
    loss.backward()
    optimizer.step()
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### è®­ç»ƒå¯¹æ¯”

| æŒ‡æ ‡ | å• Critic | Double Critic |
|------|----------|--------------|
| Foothold å­¦ä¹  | æ…¢/å·® âŒ | å¿«/å¥½ âœ… |
| Stepping Stones æˆåŠŸç‡ | ä½ | é«˜ |
| è®­ç»ƒç¨³å®šæ€§ | ä¸€èˆ¬ | æ›´å¥½ |
| è½è„šç‚¹ç²¾åº¦ | ä½ | é«˜ |

### æ§åˆ¶å°è¾“å‡º

å¯ç”¨æ—¶ä¼šçœ‹åˆ°ï¼š

```
============================================================
DOUBLE CRITIC ENABLED
  - Critic 1: Dense rewards (locomotion)
  - Critic 2: Sparse rewards (foothold)
  - Advantage weights: w1=1.0, w2=0.25
============================================================

Actor MLP: Sequential(...)
Critic MLP: Sequential(...)
Critic2 MLP (for sparse rewards): Sequential(...)  â† ç¬¬äºŒä¸ª critic
âœ“ Double Critic successfully created!                â† æˆåŠŸåˆ›å»ºæç¤º
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç¡®è®¤ double critic ç”Ÿæ•ˆï¼Ÿ

**A**: æ£€æŸ¥ä¸‰ç‚¹ï¼š
1. æ§åˆ¶å°æœ‰ "DOUBLE CRITIC ENABLED" æ¶ˆæ¯
2. **ç½‘ç»œåˆå§‹åŒ–æ˜¾ç¤º "Critic2 MLP (for sparse rewards)"** â† é‡è¦ï¼
3. è¿è¡Œæµ‹è¯•: `python test_double_critic.py`

**å¦‚æœåªçœ‹åˆ° "Critic MLP" æ²¡æœ‰ "Critic2 MLP" å’ŒæˆåŠŸæç¤º**ï¼š
- è¯´æ˜ ActorCritic æ²¡æœ‰åˆ›å»ºç¬¬äºŒä¸ª critic
- è¿è¡Œæµ‹è¯•éªŒè¯: `python test_double_critic.py`

### Q2: ä»€ä¹ˆæ—¶å€™è¯¥ç”¨ double criticï¼Ÿ

**A**: å½“ä½ æœ‰**ç¨€ç–å¥–åŠ±**ä¸”è¢«å¯†é›†å¥–åŠ±æ·¹æ²¡æ—¶ï¼š
- âœ… Stepping stones / beams åœ°å½¢ï¼ˆfoothold å¾ˆé‡è¦ï¼‰
- âœ… ä»»åŠ¡æœ‰å…³é”®ä½†ç½•è§çš„å¥–åŠ±
- âŒ æ‰€æœ‰å¥–åŠ±éƒ½å¾ˆå¯†é›†ï¼ˆæ²¡å¿…è¦ç”¨ï¼‰

### Q3: å¦‚ä½•è°ƒæ•´ w1 å’Œ w2ï¼Ÿ

**A**: è§‚å¯Ÿè®­ç»ƒæ›²çº¿ï¼š
- Foothold å¥–åŠ±ä¸å¢é•¿ â†’ å¢å¤§ w2 (å¦‚ 0.5)
- Robot ä¸ç§»åŠ¨ â†’ å‡å° w2 (å¦‚ 0.1)
- é»˜è®¤ 1.0 å’Œ 0.25 é€‚åˆå¤§å¤šæ•°æƒ…å†µ

### Q4: ä¼šå¢åŠ è®¡ç®—å¼€é”€å—ï¼Ÿ

**A**: å¼€é”€å¾ˆå°ï¼š
- é¢å¤–ä¸€ä¸ª critic ç½‘ç»œï¼ˆï½256K å‚æ•°ï¼‰
- è®­ç»ƒæ—¶é—´å¢åŠ  < 5%
- å†…å­˜å¢åŠ å¿½ç•¥ä¸è®¡

### Q5: æŠ¥é”™ "No model files found" æˆ– "list index out of range"ï¼Ÿ

**A**: è¿™æ˜¯ resume ç›¸å…³çš„é”™è¯¯ï¼š

**é—®é¢˜**: å‘½ä»¤ä¸­åŠ äº† `--resume` ä½†æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

**è§£å†³**:
```bash
# æ–¹æ³• 1: å»æ‰ --resumeï¼Œä»å¤´å¼€å§‹
python scripts/train.py --task=humanoid_stones_ppo --double_critic

# æ–¹æ³• 2: æŒ‡å®šæ­£ç¡®çš„ checkpoint
python scripts/train.py --task=humanoid_stones_ppo --double_critic \
    --resume --load_run Dec29_14-29-49_v1 --checkpoint 1300
```

### Q6: ä»æ—§æ¨¡å‹ resume æ—¶æŠ¥é”™ "Missing key(s): critic2.xxx"ï¼Ÿ

**A**: è¿™æ˜¯å› ä¸ºæ—§æ¨¡å‹æ˜¯å• critic è®­ç»ƒçš„ï¼Œæ²¡æœ‰ critic2

**ç°è±¡**:
```
RuntimeError: Missing key(s) in state_dict: 
  "critic2.0.weight", "critic2.0.bias", ...
```

**è§£å†³**: ä»£ç å·²è‡ªåŠ¨å¤„ç†ï¼ä¼šæ˜¾ç¤ºï¼š
```
âš ï¸  Warning: Loading model without critic2 (old single-critic model)
   â†’ Initializing critic2 with critic1's weights
   âœ“ Critic2 initialized from critic1 (same starting point)
```

è¿™æ˜¯**æœ€ä¼˜æ–¹æ¡ˆ**ï¼š
- âœ… Actor å’Œ Critic1 ä»æ—§æ¨¡å‹åŠ è½½ï¼ˆä¿ç•™å·²è®­ç»ƒæƒé‡ï¼‰
- âœ… **Critic2 å¤åˆ¶ Critic1 çš„æƒé‡**ï¼ˆè€Œééšæœºåˆå§‹åŒ–ï¼‰
- âœ… Critic2 ä»ä¸€ä¸ª"èªæ˜"çš„èµ·ç‚¹å¼€å§‹å­¦ä¹ ç¨€ç–å¥–åŠ±

**ä¸ºä»€ä¹ˆè¿™æ ·å¥½ï¼Ÿ**
- Critic1 å·²ç»å­¦ä¼šäº†é¢„æµ‹ç´¯ç§¯å¥–åŠ±çš„åŸºæœ¬æ¨¡å¼
- Critic2 ç»§æ‰¿è¿™äº›çŸ¥è¯†ï¼Œç„¶åé’ˆå¯¹ç¨€ç–å¥–åŠ±å¾®è°ƒ
- æ¯”ä»å¤´éšæœºå­¦ä¹ å¿«å¾—å¤šï¼

**å¦‚æœæƒ³ä»å¤´å¼€å§‹**ï¼š
```bash
python scripts/train.py --task=humanoid_stones_ppo --double_critic
```

---

## ğŸ“š æµ‹è¯•ç»“æœ

è¿è¡Œ `python test_double_critic.py` åº”è¯¥çœ‹åˆ°ï¼š

```
============================================================
ğŸ‰ ALL TESTS PASSED! ğŸ‰
============================================================

âœ“ Config test PASSED
âœ“ ActorCritic test PASSED
âœ“ RolloutStorage test PASSED
âœ“ PPO test PASSED
âœ“ Advantage computation test PASSED
  - Advantage mean: 0.0000 (should be â‰ˆ 0) âœ…
  - Advantage std: 1.0324 (should be â‰ˆ 1) âœ…
```

---

## ğŸ¯ æ€»ç»“

**Double Critic = 2 ä¸ªç‹¬ç«‹çš„ value ç½‘ç»œ + åŠ æƒç»„åˆ**

```
ä¼ ç»Ÿæ–¹æ³•:
  æ‰€æœ‰å¥–åŠ± â†’ 1 ä¸ª Critic â†’ ç¨€ç–å¥–åŠ±è¢«æ·¹æ²¡ âŒ

Double Critic:
  å¯†é›†å¥–åŠ± â†’ Critic1 (w1=1.0)  â”
                               â”œâ†’ ç»„åˆ â†’ æ›´å¥½çš„ policy âœ…
  ç¨€ç–å¥–åŠ± â†’ Critic2 (w2=0.25) â”˜
```

**å…³é”®ä¼˜åŠ¿**:
1. ç¨€ç–å¥–åŠ±ï¼ˆfootholdï¼‰å¾—åˆ°ç‹¬ç«‹å…³æ³¨
2. ç‹¬ç«‹å½’ä¸€åŒ–é˜²æ­¢å°ºåº¦é—®é¢˜
3. æå‡åœ¨å¤æ‚åœ°å½¢çš„è¡¨ç°

**ä½¿ç”¨å¾ˆç®€å•**:
```bash
# æ¨èï¼šStones Everywhere åœ°å½¢
python scripts/train.py --task=humanoid_stones_ppo --double_critic

# æˆ–å¹³å¦åœ°å½¢ï¼ˆæµ‹è¯•ç”¨ï¼‰
python scripts/train.py --task=humanoid_ppo --double_critic
```

å°±è¿™ä¹ˆç®€å•ï¼ğŸš€

